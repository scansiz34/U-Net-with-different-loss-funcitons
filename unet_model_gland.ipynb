{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from skimage import measure\n",
    "\n",
    "from loss_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_loss_proposed(weight_map):\n",
    "    def dice_loss(y_true, y_pred, smooth = 1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        weights_f = K.flatten(weight_map)\n",
    "        intersection = K.sum(weights_f * y_true_f * y_pred_f)\n",
    "        score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "        return 1. - score\n",
    "    return dice_loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_proposed(weight_map):\n",
    "    def tversky(y_true, y_pred, smooth = 1.):\n",
    "        y_true_pos = K.flatten(y_true)\n",
    "        y_pred_pos = K.flatten(y_pred)\n",
    "        weights_f = K.flatten(weight_map)\n",
    "        true_pos = K.sum(weights_f * y_true_pos * y_pred_pos)\n",
    "        false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "        false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
    "        alpha = 0.1\n",
    "        score =  (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "        return 1. - score\n",
    "    return tversky \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL PARAMS\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\" #Use 1 gpu only\n",
    "\n",
    "train = 1\n",
    "\n",
    "useBound = False #use boundary GS as target\n",
    "\n",
    "model_name = 'Unet_Tubule_Selahattin' \n",
    "imgpath = ('/media/hdd3/gunduz/lossFunction/images/Tubule/') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout, concatenate\n",
    "\n",
    "from keras.layers.core import Layer, Dense, Activation, Flatten, Reshape, Permute, Lambda\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.layers.convolutional import Convolution3D, MaxPooling3D, ZeroPadding3D\n",
    "# from keras.layers.convolutional import ZeroPadding2D\n",
    "# from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "# from keras.layers.recurrent import LSTM\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import optimizers\n",
    "# from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import objectives\n",
    "from keras import losses\n",
    "\n",
    "def weighted_mae_loss(y_true, y_pred):\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    \n",
    "    mae_loss = losses.mean_absolute_error(y_true, y_pred)\n",
    "    print(mae_loss.shape)\n",
    "    \n",
    "    weight = K.cast(y_true >= 0.5, dtype='float32') * K.variable(np.array([1.0, 1.0, 1.0, 1.0, 1.0]))\n",
    "    print(weight.shape)\n",
    "    \n",
    "    weight = K.sum(weight, axis=-1)\n",
    "    print(weight.shape)\n",
    "    \n",
    "    out = mae_loss * weight\n",
    "    print(out.shape)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def weighted_binary_crossentropy(p, w = 1):# Weighted categorical_crossentropy loss\n",
    "    #p should contain pixel-wise weigth matrix\n",
    "    #w is a floating point constant\n",
    "    \n",
    "    def loss(y_true, y_pred):        \n",
    "        cc_loss = objectives.binary_crossentropy(y_true, y_pred)\n",
    "        loss = (w * p * cc_loss)\n",
    "        return K.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def custom_loss(weight_map):\n",
    "    def loss(y_true, y_pred):\n",
    "        ce = losses.binary_crossentropy(y_true, y_pred)\n",
    "        return np.dot(weight_map, ce)\n",
    "    return loss \n",
    "\n",
    "\n",
    "#Segmentation Model\n",
    "def unet_model(input_height=528, input_width=784, k=32, nChannels = 3, dropout_rate = 0.2):\n",
    "    \n",
    "    optimizer = optimizers.Adadelta()\n",
    "    image = Input(shape= (input_height, input_width, nChannels))\n",
    "    weights = Input(shape= (input_height, input_width))\n",
    "    \n",
    "    conv1 = Conv2D(k,(3, 3), activation='relu', padding='same')(image)\n",
    "    conv1 = Dropout(dropout_rate)(conv1)\n",
    "    conv1 = Conv2D(k, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(k*2,(3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Dropout(dropout_rate)(conv2)\n",
    "    conv2 = Conv2D(k*2,(3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(k*4,(3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Dropout(dropout_rate)(conv3)\n",
    "    conv3 = Conv2D(k*4,(3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(k*8,(3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Dropout(dropout_rate)(conv4)\n",
    "    conv4 = Conv2D(k*8,(3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Conv2D(k*16,(3, 3), activation='relu', padding='same')(pool4) \n",
    "    conv5 = Dropout(dropout_rate)(conv5)\n",
    "    conv5 = Conv2D(k*16,(3, 3), activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    conv5_ = Conv2D(k*32,(3, 3), activation='relu', padding='same')(conv5) \n",
    "    conv5_ = Dropout(dropout_rate)(conv5_) #new\n",
    "    conv5_ = Conv2D(k*32,(3, 3), activation='relu', padding='same')(conv5_)\n",
    "    \n",
    "    conv5_2 = Conv2D(k*32,(3, 3), activation='relu', padding='same')(conv5_)\n",
    "    conv5_2 = Dropout(dropout_rate)(conv5_2) #new 2\n",
    "    conv5_2 = Conv2D(k*32,(3, 3), activation='relu', padding='same')(conv5_2)\n",
    "    \n",
    "    concat_1 = concatenate([conv5_2, conv5_], axis=3) #new \n",
    "    conv6_2 = Conv2D(k*16,(3, 3), activation='relu', padding='same')(concat_1)\n",
    "    conv6_2 = Dropout(dropout_rate)(conv6_2) \n",
    "    conv6_2 = Conv2D(k*16, (3, 3), activation='relu', padding='same')(conv6_2)\n",
    "\n",
    "    concat_2 = concatenate([conv6_2, conv5], axis=3)\n",
    "    conv6_ = Conv2D(k*16,(3, 3), activation='relu', padding='same')(concat_2)\n",
    "    conv6_ = Dropout(dropout_rate)(conv6_)\n",
    "    conv6_ = Conv2D(k*16, (3, 3), activation='relu', padding='same')(conv6_)\n",
    " \n",
    "    up1 = concatenate([UpSampling2D(size=(2, 2))(conv6_), conv4], axis=3)    \n",
    "    conv6 = Conv2D(k*8,(3, 3), activation='relu', padding='same')(up1)\n",
    "    conv6 = Dropout(dropout_rate)(conv6)\n",
    "    conv6 = Conv2D(k*8, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up2 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(k*4,(3, 3), activation='relu', padding='same')(up2)\n",
    "    conv7 = Dropout(dropout_rate)(conv7)\n",
    "    conv7 = Conv2D(k*4, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up3 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(k*2,(3, 3), activation='relu', padding='same')(up3)\n",
    "    conv8 = Dropout(dropout_rate)(conv8)\n",
    "    conv8 = Conv2D(k*2, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up4 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(k,(3, 3), activation='relu', padding='same')(up4)\n",
    "    conv9 = Dropout(dropout_rate)(conv9)\n",
    "    conv9 = Conv2D(k, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    output = Conv2D(1, (1,1), activation = 'sigmoid')(conv9)\n",
    "   \n",
    "\n",
    "    model = Model(inputs = [image, weights], outputs = output )\n",
    "    \n",
    "#     loss1 = weighted_binary_crossentropy(weights, w = 1)\n",
    "#     loss2 = weighted_categorical_crossentropy(out2_weights, w = 1)\n",
    "    loss1 = custom_loss(weights)\n",
    "#     loss2 = custom_loss_proposed(weights)\n",
    "   \n",
    "    \n",
    "    model.compile(loss = tversky_loss, optimizer = optimizer , metrics=['binary_accuracy'])\n",
    "    \n",
    "#     model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['binary_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read images\n",
    "from glob import glob\n",
    "import scipy\n",
    "from scipy import io\n",
    "\n",
    "gs_images = []\n",
    "images = []\n",
    "wei_images = []\n",
    "\n",
    "im_width = 784\n",
    "im_height = 528\n",
    "\n",
    "path_images = sorted(glob('/media/hdd3/gunduz/cansari/datasets/tubule/images/*'))\n",
    "\n",
    "for img_path in path_images:\n",
    "    \n",
    "    print(img_path)\n",
    "    img = scipy.misc.imread(img_path)\n",
    "    gs = scipy.io.loadmat(img_path.replace(\"images\",\"gold_standard_segm\").replace(\".jpg\",\".mat\"))\n",
    "    gs = (gs[\"goldSeg\"] / 255).astype(np.int_)\n",
    "    images.append(img)\n",
    "    gs_images.append(gs)\n",
    "    \n",
    "    wei = np.zeros(shape = (im_height,im_width), dtype=float)\n",
    "    weight = sum(sum(gs[:,:,0])) / (im_height*im_width)\n",
    "    \n",
    "    if weight < 0.5:\n",
    "        wei[gs[:,:,0] > 0] = (1.0 - weight) + 0.0001\n",
    "        wei[gs[:,:,0] == 0] = (weight) + 0.0001\n",
    "        wei_images.append(wei)\n",
    "        \n",
    "    else:\n",
    "        wei[gs[:,:,0] > 0] = (weight) + 0.0001\n",
    "        wei[gs[:,:,0] == 0] = (1.0 - weight) + 0.0001\n",
    "        wei_images.append(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(85):\n",
    "    fig=plt.figure(figsize=(10, 3))\n",
    "\n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.imshow(images[i])  \n",
    "    \n",
    "    gs = gs_images[i]\n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(gs[:,:,0], cmap=\"gray\")\n",
    "    \n",
    "    wei = wei_images[i]\n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.imshow(wei, cmap=\"gray\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gs_images[2]\n",
    "plt.imshow(gs[:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.shape)\n",
    "print(gs[:,:,0])\n",
    "print(sum(sum(gs[:,:,0])))\n",
    "weight = sum(sum(gs[:,:,0])) / (im_height*im_width)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = wei_images[2] \n",
    "plt.imshow(wei, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(wei[400,100])\n",
    "print(wei[200,100])\n",
    "print(wei[400,100] + wei[200,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating train, validation and test sets\n",
    "\n",
    "import random \n",
    "random.seed(1)\n",
    "randomlist = random.sample(range(0, 85), 85)\n",
    "\n",
    "trSize = 60\n",
    "valSize = 15\n",
    "tsSize = 10\n",
    "\n",
    "images_train =  []\n",
    "images_val =  []\n",
    "images_test =  []\n",
    "\n",
    "gs_train =  []\n",
    "gs_val =  []\n",
    "gs_test =  []\n",
    "\n",
    "wei_train = []\n",
    "wei_val = []\n",
    "wei_test = []\n",
    "\n",
    "for index in randomlist:\n",
    "    trC = len(images_train)\n",
    "    valC = len(images_val)\n",
    "    tsC = len(images_test)\n",
    "\n",
    "    if trC < trSize:\n",
    "        images_train.append(images[index])\n",
    "        gs_train.append(gs_images[index])\n",
    "        wei_train.append(wei_images[index])\n",
    "        \n",
    "    if trC == trSize and valC < valSize:\n",
    "        images_val.append(images[index])\n",
    "        gs_val.append(gs_images[index])\n",
    "        wei_val.append(wei_images[index])\n",
    "        \n",
    "    if trC == trSize and valC == valSize:\n",
    "        images_test.append(images[index])\n",
    "        gs_test.append(gs_images[index])\n",
    "        wei_test.append(wei_images[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(images_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gs_val[0]\n",
    "plt.imshow(gs[:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = wei_val[0] \n",
    "plt.imshow(wei, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Datasets\n",
    "\n",
    "images_train =   np.asarray(images_train)\n",
    "gs_train = np.asarray(gs_train)\n",
    "wei_train = np.asarray(wei_train)\n",
    "\n",
    "images_val =   np.asarray(images_val)\n",
    "gs_val = np.asarray(gs_val)\n",
    "wei_val = np.asarray(wei_val)\n",
    "    \n",
    "images_test =   np.asarray(images_test)\n",
    "gs_test = np.asarray(gs_test)\n",
    "wei_test = np.asarray(wei_test)\n",
    "\n",
    "#normalize\n",
    "\n",
    "def normalizeImg(img):\n",
    "    norm_img = np.zeros(img.shape)\n",
    "    for i in range(3):\n",
    "        norm_img[:,:,i] = (img[:,:,i] - img[:,:,i].mean()) / (img[:,:,i].std())\n",
    "    return norm_img\n",
    "\n",
    "x_train = np.zeros(images_train.shape)\n",
    "x_valid = np.zeros(images_val.shape)\n",
    "x_test = np.zeros(images_test.shape)\n",
    "    \n",
    "for i in range(images_train.shape[0]):\n",
    "    x_train[i,:,:,:] = normalizeImg(images_train[i,:,:,:])\n",
    "    \n",
    "for i in range(images_val.shape[0]):\n",
    "    x_valid[i,:,:,:] = normalizeImg(images_val[i,:,:,:])\n",
    "    \n",
    "for i in range(images_test.shape[0]):\n",
    "    x_test[i,:,:,:] = normalizeImg(images_test[i,:,:,:])\n",
    "\n",
    "y_train = (gs_train > 0).astype(np.int_)\n",
    "y_valid = (gs_val > 0).astype(np.int_)\n",
    "y_test = (gs_test > 0).astype(np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train[:,:,:,0].reshape(trSize,im_height ,im_width, 1)\n",
    "y_valid=y_valid[:,:,:,0].reshape(valSize,im_height ,im_width, 1) \n",
    "y_test=y_test[:,:,:,0].reshape(tsSize,im_height ,im_width, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)\n",
    "print(wei_train.shape)\n",
    "print(wei_val.shape)\n",
    "print(wei_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei_train_2 = np.full((wei_train.shape), 0.1)\n",
    "wei_val_2 = np.full((wei_val.shape), 0.1)\n",
    "\n",
    "\n",
    "for i in range (len(y_train)):\n",
    "    out_masks = y_train[i,:,:,0] == 1\n",
    "    wei_train_2[i][out_masks] = 0.9\n",
    "\n",
    "for i in range (len(y_valid)):\n",
    "    out_masks = y_valid[i,:,:,0] == 1\n",
    "    wei_val_2[i][out_masks] = 0.9\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(images_train[i,:,:,:])\n",
    "\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(y_train[i,:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "index = []\n",
    "for i in range(60):\n",
    "    if y_train[i,:,:,0].max() == 0:\n",
    "        count += 1\n",
    "        index.append(i)\n",
    "print(count)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = ('/media/hdd3/gunduz/lossFunction/models/%s.hdf5' % model_name) \n",
    "print(model_path)\n",
    "\n",
    "model =  unet_model(input_height=528, input_width=784, k=32, nChannels = 3, dropout_rate = 0.2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(train):\n",
    "\n",
    "    #Train the model\n",
    "    checkpointer = ModelCheckpoint(model_path, monitor='val_loss', verbose=0,\n",
    "                                   save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    earlystopper = EarlyStopping(patience = 100, verbose=1)\n",
    "    \n",
    "    hist = model.fit(x = [x_train, wei_train], y = y_train, \n",
    "                     validation_data = ([x_valid, wei_val], y_valid),\n",
    "                     epochs = 1000, batch_size = 2, shuffle=True, # sample_weight = {'output_': wei_train[:,:,0]},                \n",
    "                     callbacks=[checkpointer, earlystopper])\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure()\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "\n",
    "wei_test_1 = np.ones((x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
    "wei_test_1 = wei_test_1 / 2\n",
    "pr_test = model.predict([x_test, wei_test_1], batch_size =4, verbose = 1)\n",
    "\n",
    "print('pr_test.shape : ', pr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fuction = \"wbce_\"\n",
    "# loss_fuction = \"focal_b2_\"\n",
    "loss_fuction = \"tversky_2_\"\n",
    "# loss_fuction = \"focal_tversky_\"\n",
    "# loss_fuction = \"compound_\"\n",
    "# loss_fuction = \"bce_\"\n",
    "\n",
    "for i in range(10):\n",
    "    scipy.io.savemat(imgpath + loss_fuction + str(i) + \".mat\", {'im': images_test[i,:,:,:], 'gs': gs_test[i,:,:,0], 'pr': pr_test[i,:,:,0]})\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "\n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.imshow(images_test[i,:,:,:] / 255.0)\n",
    "\n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    #plt.imshow(gs_test[i,:,0].reshape(1024, 1024), cmap=\"gray\")\n",
    "    plt.imshow(y_test[i,:,:,0], cmap=\"gray\")\n",
    "\n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    #plt.imshow(pr_test[i,:,0].reshape(1024, 1024), cmap=\"gray\")\n",
    "    plt.imshow(pr_test[i,:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_masks = pr_test[:,:,:,0] > 0.5\n",
    "new_data = np.zeros(out_masks.shape)\n",
    "new_data[out_masks] = 1\n",
    "\n",
    "sc = []\n",
    "for i in range (len(new_data)):\n",
    "    TP, FP, TN, FN = perf_measure(gs_test[i,:,:,0].astype(int), new_data[i,:,:].astype(int))\n",
    "    sc.append([dice(TP, FP, FN), sensitivity(TP, FN), specificity(FP, TN), precision(TP, FP)])\n",
    "sc = np.asarray(sc)\n",
    "\n",
    "sc.mean(axis=0).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading predicted images\n",
    "\n",
    "img = []\n",
    "gold_std = []\n",
    "predict_segm = []\n",
    "\n",
    "path_images = sorted(glob(imgpath + \"*\"))\n",
    "\n",
    "for img_path in path_images:    \n",
    "    \n",
    "    image = scipy.io.loadmat(img_path)\n",
    "    img.append(image[\"im\"])\n",
    "    gold_std.append(image[\"gs\"])\n",
    "    predict_segm.append(image[\"pr\"])\n",
    "    \n",
    "img = np.asarray(img)\n",
    "gold_std = np.asarray(gold_std)\n",
    "predict_segm = np.asarray(predict_segm)\n",
    "    \n",
    "# for i in range(len(img)):\n",
    "#     fig=plt.figure(figsize=(10, 10))\n",
    "\n",
    "#     fig.add_subplot(1, 3, 1)\n",
    "#     plt.imshow(img[i,:,:,:] / 255.0)\n",
    "\n",
    "#     fig.add_subplot(1, 3, 2)\n",
    "#     #plt.imshow(gs_test[i,:,0].reshape(1024, 1024), cmap=\"gray\")\n",
    "#     plt.imshow(gold_std[i,:,:], cmap=\"gray\")\n",
    "\n",
    "#     fig.add_subplot(1, 3, 3)\n",
    "#     #plt.imshow(pr_test[i,:,0].reshape(1024, 1024), cmap=\"gray\")\n",
    "#     plt.imshow(pr_weighted_ce[i,:,:], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_masks = predict_segm[:,:,:] > 0.5\n",
    "new_data = np.zeros(out_masks.shape)\n",
    "new_data[out_masks] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(30,40):\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "\n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.imshow(img[i,:,:,:] / 255.0)\n",
    "\n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(gold_std[i,:,:], cmap=\"gray\")\n",
    "\n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.imshow(new_data[i,:,:], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "def connected_comp(img):\n",
    "    \n",
    "    num_labels, labels = cv.connectedComponents(img.astype(\"uint8\"))\n",
    "\n",
    "    # Map component labels to hue val, 0-179 is the hue range in OpenCV\n",
    "    label_hue = np.uint8(179*labels/np.max(labels))\n",
    "    blank_ch = 255*np.ones_like(label_hue)\n",
    "    labeled_img = cv.merge([label_hue, blank_ch, blank_ch])\n",
    "\n",
    "    # Converting cvt to BGR\n",
    "    labeled_img = cv.cvtColor(labeled_img, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    # set bg label to black\n",
    "    labeled_img[label_hue==0] = 0\n",
    "    \n",
    "    return labeled_img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10,20):\n",
    "    fig=plt.figure(figsize=(20, 10))\n",
    "\n",
    "    fig.add_subplot(1, 6, 1)\n",
    "    plt.imshow(img[i,:,:,:] / 255.0), plt.title(\"Image\"), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "    fig.add_subplot(1, 6, 2)\n",
    "    plt.imshow(gold_std[i,:,:], cmap=\"gray\"), plt.title(\"Ground Truth\"), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "    fig.add_subplot(1, 6, 3)\n",
    "    labeled_img = connected_comp(new_data[i,:,:])\n",
    "    plt.imshow(cv.cvtColor(labeled_img, cv.COLOR_BGR2RGB)), plt.xticks([]), plt.yticks([]), plt.title(\"Focal Loss\")\n",
    "    \n",
    "    fig.add_subplot(1, 6, 4)\n",
    "    labeled_img = connected_comp(new_data[i+10,:,:])\n",
    "    plt.imshow(cv.cvtColor(labeled_img, cv.COLOR_BGR2RGB)), plt.xticks([]), plt.yticks([]), plt.title(\"Focal Tversky\")\n",
    "    \n",
    "    fig.add_subplot(1, 6, 5)\n",
    "    labeled_img = connected_comp(new_data[i+20,:,:])\n",
    "    plt.imshow(cv.cvtColor(labeled_img, cv.COLOR_BGR2RGB)), plt.xticks([]), plt.yticks([]), plt.title(\"Tversky (β=0.88)\")\n",
    "    \n",
    "    fig.add_subplot(1, 6, 6)\n",
    "    labeled_img = connected_comp(new_data[i+30,:,:])\n",
    "    plt.imshow(cv.cvtColor(labeled_img, cv.COLOR_BGR2RGB)), plt.xticks([]), plt.yticks([]), plt.title(\"Weighted Binary CE\")\n",
    "    \n",
    "# plt.savefig('prediction.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_target, y_pred):\n",
    "    \n",
    "    TP = np.sum((y_target==1) & (y_pred==1))\n",
    "    FP = np.sum((y_pred==1) & (y_target!=y_pred))    \n",
    "    TN = np.sum((y_target==0) & (y_pred==0))\n",
    "    FN = np.sum((y_pred==0) & (y_target!=y_pred))\n",
    "    \n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def sensitivity(TP, FN):\n",
    "    return (TP / (TP+FN)) \n",
    "\n",
    "def specificity(FP, TN):\n",
    "    return (TN / (TN+FP))\n",
    "\n",
    "def accuracy(TP, FP, TN, FN):\n",
    "    return ((TP+TN) / (TP + FP + TN + FN))\n",
    "\n",
    "def precision(TP, FP):\n",
    "    return (TP / (TP+FP))\n",
    "\n",
    "def dice(TP, FP, FN):\n",
    "    return ((2*TP) / (2*TP+FP+FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for i in range (len(new_data)):\n",
    "    TP, FP, TN, FN = perf_measure(gold_std[i,:,:].astype(int), new_data[i,:,:].astype(int))\n",
    "    metrics.append([dice(TP, FP, FN), sensitivity(TP, FN), specificity(FP, TN), precision(TP, FP)])\n",
    "metrics = np.asarray(metrics)\n",
    "\n",
    "metrics.mean(axis=0).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(len(metrics)/10)):\n",
    "    index = i * 10 \n",
    "    print(metrics[index:index+10].mean(axis=0).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[10:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
